{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f09d6a-4bd2-46c5-8a1f-09bcec47580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time                                               text  \\\n",
      "0     2020-09-21 08:32:00  한화정밀기계와 SK하이닉스가 공동 개발한 다이 본더 이미지  한화정밀기계가 SK하이...   \n",
      "1     2020-09-21 08:35:00  한화정밀기계가 SK하이닉스와 함께 국산화에 성공한 반도체 후공정 핵심 장비 ‘다이 ...   \n",
      "2     2020-09-21 08:48:00  한화정밀기계와 SK하이닉스(000660)가 국산화한 반도체 후공정 핵심 장비 ‘다이...   \n",
      "3     2020-09-21 11:39:00  글로벌 투자은행(IB) 씨티증권이 국내 반도체 투톱의 목표주가를 상향 조정하고 나섰...   \n",
      "4     2020-09-21 17:25:00  SK하이닉스가 협력사에 대한 지원 확대 등을 통해 반도체 소재·부품·장비(소부장) ...   \n",
      "...                   ...                                                ...   \n",
      "1322  2021-09-15 16:47:00  삼성전자가 동반성장위원회가 선정하는 '2020년도 동반성장지수 평가'에서 국내 기업...   \n",
      "1323  2021-09-15 19:03:00  SK하이닉스의 10나노급 D램 제품을 주로 생산할 M16 전경. 사진=뉴스1  무디...   \n",
      "1324  2021-09-16 09:51:00  이미지=게티이미지뱅크코스피지수가 장 초반 약보합세를 나타내고 있다. 간밤 미 뉴욕증...   \n",
      "1325  2021-09-16 10:05:00  SK하이닉스가 SKMS(SK Management System)담아 제작한 마스코트 ...   \n",
      "1326  2021-09-16 14:04:00  SK하이닉스가 외국인과 기관의 매도세에 주가가 3% 이상 하락하고 있다. 16일 오...   \n",
      "\n",
      "      label  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "...     ...  \n",
      "1322      0  \n",
      "1323      0  \n",
      "1324      0  \n",
      "1325      0  \n",
      "1326      0  \n",
      "\n",
      "[1327 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "datapath = '../data/pre_*.csv'\n",
    "for filename in glob(datapath):\n",
    "    df = pd.read_csv(filename, usecols=['time','text','label'])\n",
    "    print(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbabaaf-e8aa-4116-ac74-b145df61852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_data(url, max_tries=5):\n",
    "    remaining_tries = int(max_tries)\n",
    "    while remaining_tries > 0:\n",
    "        try:\n",
    "            return requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        except requests.exceptions:\n",
    "            sleep(1)\n",
    "        remaining_tries = remaining_tries - 1\n",
    "    raise ResponseTimeout()\n",
    "def make_news_page_url(category_url, start_year, end_year, start_month, end_month):\n",
    "    made_urls = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        target_start_month = start_month\n",
    "        target_end_month = end_month\n",
    "\n",
    "        if start_year != end_year:\n",
    "            if year == start_year:\n",
    "                target_start_month = start_month\n",
    "                target_end_month = 12\n",
    "            elif year == end_year:\n",
    "                target_start_month = 1\n",
    "                target_end_month = end_month\n",
    "            else:\n",
    "                target_start_month = 1\n",
    "                target_end_month = 12\n",
    "\n",
    "        for month in range(target_start_month, target_end_month + 1):\n",
    "            for month_day in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "                if len(str(month)) == 1:\n",
    "                    month = \"0\" + str(month)\n",
    "                if len(str(month_day)) == 1:\n",
    "                    month_day = \"0\" + str(month_day)\n",
    "\n",
    "                # 날짜별로 Page Url 생성\n",
    "                url = category_url + str(year) + str(month) + str(month_day)\n",
    "\n",
    "                # totalpage는 네이버 페이지 구조를 이용해서 page=10000으로 지정해 totalpage를 알아냄\n",
    "                # page=10000을 입력할 경우 페이지가 존재하지 않기 때문에 page=totalpage로 이동 됨 (Redirect)\n",
    "                totalpage = ArticleParser.find_news_totalpage(url + \"&page=10000\")\n",
    "                for page in range(1, totalpage + 1):\n",
    "                    made_urls.append(url + \"&page=\" + str(page))\n",
    "    return made_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124e688b-fa04-4dc9-a65c-2e8127528a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90'\n",
    "url2 = 'https://search.naver.com/search.naver?where=news&query=%ED%8F%AC%EC%8A%A4%EC%BD%94&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=20200909&de=20190808,a:all&start=3991'\n",
    "url3 = 'https://search.naver.com/search.naver?where=news&query=%ED%8F%AC%EC%8A%A4%EC%BD%94&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds=20210906&de=20210920&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20210906to20210920&is_sug_officeid=0'\n",
    "url4 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%ED%8F%AC%EC%8A%A4%EC%BD%94&sort=1&photo=0&field=0&pd=3&ds=20210915&de=20210921&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210915to20210921,a:all&start=1371'\n",
    "url5 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%ED%8F%AC%EC%8A%A4%EC%BD%94&sort=1&photo=0&field=0&pd=3&ds=20210908&de=20210921&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210908to20210921,a:all&start=3991'\n",
    "lim_url = 'https://search.naver.com/search.naver?where=news&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=20210908&de=20210907&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210907to20210908,a:all&start=3981'\n",
    "request = get_url_data(lim_url)\n",
    "print(request)\n",
    "document = BeautifulSoup(request.content, 'html.parser')\n",
    "# print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f3ae3ecf-713a-4a48-bde0-3defc2361a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_post = document.select('.api_subject_bx .group_news li .news_area')\n",
    "temp_post2 = document.select('.api_subject_bx .group_news li .news_area .info_group span.info')\n",
    "temp_post3 = document.select('.sc_page_inner .btn')\n",
    "# print(temp_post3)\n",
    "# import datetime\n",
    "post_urls = []\n",
    "# for line in temp_post:\n",
    "#     print(line.a.get('data-url'))\n",
    "# print(datetime.datetime.strptime(temp_post2[-1].text, '%Y.%m.%d.') - datetime.timedelta(1))\n",
    "\n",
    "if '일 전' in temp_post2[-1].text:\n",
    "    print(int(temp_post2[-1].text[:-3]))\n",
    "#     de = de - int(temp_post2[-1].text[:-3])\n",
    "# for line in temp_post3:\n",
    "# #     print(line)\n",
    "#     post_urls.append(line.text)\n",
    "# print(post_urls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6973531a-bcdd-4951-a084-8dbac9b3023c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20210907'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = '20210909'\n",
    "\n",
    "last_time = datetime.datetime.strptime(temp_post2[-1].text, '%Y.%m.%d.')\n",
    "last_time >datetime.datetime.strptime(ds, '%Y%m%d')\n",
    "newds = last_time - datetime.timedelta(1)\n",
    "newds.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a52023a5-e235-405e-b88d-2a4a33446195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1521\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=153&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">153</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1531\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=154&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">154</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1541\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=155&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">155</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1551\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=156&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">156</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1561\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=157&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">157</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1571\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=158&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">158</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1581\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=159&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">159</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1591\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=160&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">160</a>, <a aria-pressed=\"false\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1601\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=161&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">161</a>, <a aria-pressed=\"true\" class=\"btn\" href=\"?where=news&amp;sm=tab_pge&amp;query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&amp;sort=1&amp;photo=0&amp;field=0&amp;pd=3&amp;ds=20210907&amp;de=20210908&amp;mynews=0&amp;office_type=0&amp;office_section_code=0&amp;news_office_checked=&amp;nso=so:dd,p:from20210907to20210908,a:all&amp;start=1611\" onclick=\"return goOtherCR(this, 'a=nws.paging&amp;r=161&amp;i=&amp;u='+urlencode(urlexpand(this.href)));\" role=\"button\">162</a>]\n",
      "https://search.naver.com/search.naver?where=news&query=삼성전자&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=20210907&de=20210908&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210907to20210908,a:all&start=1621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['153', '154', '155', '156', '157', '158', '159', '160', '161', '162']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = '20210907'\n",
    "de = '20210908'\n",
    "baseurl = f'https://search.naver.com/search.naver?where=news&query=삼성전자' \\\n",
    "            f'&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds={ds}&de={de}' \\\n",
    "            f'&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,' \\\n",
    "            f'p:from{ds}to{de},a:all&start='\n",
    "\n",
    "request = get_url_data(baseurl+'1611') # 마지막 400페이지 \n",
    "document = BeautifulSoup(request.content, 'html.parser')\n",
    "last_post = document.select('.sc_page_inner .btn')\n",
    "print(last_post)\n",
    "post_urls = []\n",
    "print(baseurl+'1621')\n",
    "for line in last_post:\n",
    "    post_urls.append(line.text)\n",
    "#     print(post_urls[-1])\n",
    "post_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50fcabb7-2273-41b9-aa62-b23c94efd85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "161\n",
      "162\n",
      "162\n",
      "162\n",
      "162\n",
      "162\n",
      "162\n",
      "----- 162\n",
      "https://search.naver.com/search.naver?where=news&query=삼성전자&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=20210907&de=20210908&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210907to20210908,a:all&start=1621\n"
     ]
    }
   ],
   "source": [
    "last_page_ind = ['00']\n",
    "for i in range(155,400):\n",
    "    url = f'https://search.naver.com/search.naver?where=news&query=삼성전자&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=20210907&de=20210908&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210907to20210908,a:all&start={i}1'\n",
    "    re = get_url_data(url)\n",
    "    do = BeautifulSoup(re.content, 'html.parser')\n",
    "    po = do.select('.sc_page_inner .btn')\n",
    "    leng = len(last_page_ind)\n",
    "    try:\n",
    "        print(last_page_ind[-1])\n",
    "        last_page_ind.append(po[-1].text)\n",
    "    except:\n",
    "        print('-'*5,last_page_ind[-1])\n",
    "        print(url)\n",
    "        break\n",
    "#     if last_page_ind[0]!=last_page_ind[1]:\n",
    "#         last_page_ind.pop(0)\n",
    "#     else: #같다는건 마지막 페이지에 도달했다는 뜻\n",
    "#         print(last_page_ind)\n",
    "#     print(last_page_ind)\n",
    "#     print(po[-1].text)\n",
    "#     for idx,i in enumerate(po):\n",
    "#         print(i.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a3bb8d-916f-4f5b-a6f8-787665e7c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "검색 한 번당 최대 출력 페이지는 400페이지이다. 따라서 최신순 sorting 날짜를 조절해가며 나눠서 Crawling한다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "hello\n",
      "2021.08.17.\n",
      "쿼리 :삼성전자, 시작 : 20210813, 종료 : 20210816\n",
      "https://search.naver.com/search.naver?where=news&query=삼성전자&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=20210813&de=20210822&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from20210813to20210822,a:all&start=3991\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "318 막지막 여기페이지는 여기올\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "# def find_last_page_num(url):\n",
    "#     global last_page_num\n",
    "#     try:\n",
    "#         request = get_url_data(url+'100001') # 마지막 400페이지 \n",
    "# #         print(url)\n",
    "#         document = BeautifulSoup(request.content, 'html.parser')\n",
    "#         last_post = document.select('.sc_page_inner .btn')\n",
    "#         post_urls = []\n",
    "#         for line in last_post:\n",
    "#             post_urls.append(line.text)\n",
    "#         last_page_num = post_urls[-1]\n",
    "#     except IndexError:\n",
    "#         print('hello')\n",
    "#         for i in range(399):\n",
    "#             request1 = get_url_data(url+f'{i}1')\n",
    "#             request2 = get_url_data(url+f'{i+1}1')\n",
    "#             document1 = BeautifulSoup(request.content, 'html.parser')\n",
    "#             document2 = BeautifulSoup(request.content, 'html.parser')\n",
    "#             last_post = document2.select('.sc_page_inner .btn')\n",
    "#             print(url+f'{i+1}1')\n",
    "#             print(last_post)\n",
    "#             if last_post==[]:      \n",
    "#                 print('들어감')\n",
    "#                 post_urls = []\n",
    "#                 for line in document1.select('.sc_page_inner .btn'):\n",
    "#                     print(line)\n",
    "#                     post_urls.append(line.text)\n",
    "# #                 print(\n",
    "#                 last_page_num = post_urls[-1]\n",
    "#     return int(last_page_num)\n",
    "\n",
    "def make_news_page_url(made_urls, query, ds, de, jump=1):\n",
    "    #####################################################################\n",
    "    # 검색 한 번당 최대 출력 페이지는 400페이지이다. \n",
    "    # 400 페이지에 도달할 시 마지막 기사의 날짜를 파싱하여 해당 날짜보다 하루 전 기사부터 다시 크롤링 시작 \n",
    "    # 따라서 최신순 sorting 날짜를 조절해가며 나눠서 Crawling한다\n",
    "    # 끝날무렵 400 미만의 페이지에서 더이상 기사가 없을 시에 이전 페이지까지의 url 리스트를 반환\n",
    "    ######################################################################\n",
    "    cnt = 0\n",
    "    baseurl = f'https://search.naver.com/search.naver?where=news&query={query}' \\\n",
    "            f'&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds={ds}&de={de}' \\\n",
    "            f'&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,' \\\n",
    "            f'p:from{ds}to{de},a:all&start='\n",
    "#     last_page_num = find_last_page_num(baseurl)\n",
    "    while True: #언제까지하냐면 다음 넘어가기 페이지가 존재할때까지  \n",
    "        \n",
    "#         try:\n",
    "            for page in range(0, 400, jump): \n",
    "#                 if page%10==0:\n",
    "#                     print(page)\n",
    "                # 최대 검색 가능 페이지 : 400페이지 \n",
    "                # jump : 400페이지 너무 느리거나 많다 싶으면 jump를 통해 페이지 넘버 조절 가능 \n",
    "                url = baseurl + f'{page}1'\n",
    "#                 print(url)\n",
    "                made_urls.append(url)\n",
    "                request = get_url_data(url) # 마지막 페이지\n",
    "                document = BeautifulSoup(request.content, 'html.parser')                \n",
    "                last_time_post = document.select('.api_subject_bx .group_news li .news_area .info_group span.info')\n",
    "                if page == 399 and len(last_time_post)>=1:\n",
    "                    print('hello')\n",
    "                    print(last_time_post[-1].text)\n",
    "                    last_time =  datetime.datetime.strptime(last_time_post[-1].text, '%Y.%m.%d.')\n",
    "                    \n",
    "                    if last_time > datetime.datetime.strptime(ds, '%Y%m%d'):\n",
    "                        # 마지막 페이지의 마지막 기사 시간이 검색 시작 일자보다 클때 한 번 더 반복 \n",
    "                        de = last_time - datetime.timedelta(1)\n",
    "                        de = de.strftime('%Y%m%d')\n",
    "                        print(f'쿼리 :{query}, 시작 : {ds}, 종료 : {de}')\n",
    "                        print(url)\n",
    "                        return make_news_page_url(made_urls, query, ds, de, jump=1)\n",
    "                    else:\n",
    "                        # \n",
    "                        return made_urls\n",
    "                elif len(last_time_post)==0:\n",
    "                    print(page-1,'막지막 여기페이지는 여기올')\n",
    "                    return made_urls\n",
    "print('-'*100)\n",
    "print('검색 한 번당 최대 출력 페이지는 400페이지이다. 따라서 최신순 sorting 날짜를 조절해가며 나눠서 Crawling한다')\n",
    "print('-'*100)\n",
    "len(make_news_page_url([], '삼성전자', '20210813', '20210822'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f1c3ecb8-a802-49b7-8278-452403070c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "post6 = document.select('.sc_page .btn_next')\n",
    "for line in post6:\n",
    "    print(line)\n",
    "    print(line.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d780a0a2-e1f3-477c-8abc-f85a6eb78ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링 중...\n",
      "17시간 전\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':', '시') + '분'\n",
    "\n",
    "query = '삼성전자'#input('검색 키워드를 입력하세요 : ')\n",
    "news_num = 10#int(input('총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : '))\n",
    "query = query.replace(' ', '+')\n",
    "\n",
    "news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "\n",
    "req = requests.get(news_url.format(query))\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "news_dict = {}\n",
    "idx = 0\n",
    "cur_page = 1\n",
    "\n",
    "print()\n",
    "print('크롤링 중...')\n",
    "\n",
    "while idx < news_num:\n",
    "    ### 네이버 뉴스 웹페이지 구성이 바뀌어 태그명, \n",
    "    # class 속성 값 등을 수정함(20210126) ###\n",
    "\n",
    "    table = soup.find('ul', {'class': 'list_news'})\n",
    "    li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "    area_list = [li.find('div', {'class': 'news_area'}) for li in li_list]\n",
    "#     print(area_list)\n",
    "#     break\n",
    "    a_list = [area.find('a', {'class': 'news_tit'}) for area in area_list]\n",
    "    i_list = [area.find('span', {'class': 'info'}) for area in area_list]\n",
    "    print(i_list[0].text)\n",
    "    break\n",
    "    for n in a_list[:min(len(a_list), news_num - idx)]:\n",
    "        news_dict[idx] = {'title': n.get('title'),\n",
    "                          'url': n.get('href')}\n",
    "        print(n.get('href'))\n",
    "    break\n",
    "#         idx += 1\n",
    "\n",
    "    cur_page += 1\n",
    "\n",
    "    pages = soup.find('div', {'class': 'sc_page_inner'})\n",
    "    next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
    "\n",
    "    req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "# print('크롤링 완료')\n",
    "\n",
    "# print('데이터프레임 변환')\n",
    "# news_df = DataFrame(news_dict).T\n",
    "\n",
    "# folder_path = os.getcwd()\n",
    "# # xlsx_file_name = '네이버뉴스_{}_{}.csv'.format(query, date)\n",
    "\n",
    "# # news_df.to_csv(xlsx_file_name)\n",
    "# print(news_df)\n",
    "# # print('엑셀 저장 완료 | 경로 : {}\\\\{}'.format(folder_path, xlsx_file_name))\n",
    "# # os.startfile(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch171",
   "language": "python",
   "name": "torch171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
